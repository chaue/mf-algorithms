{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-negative matrix factorization (NMF) is a simple yet effective method to decompose a matrix into a product of two non-negative matrices(that is sparse matrices with all non-negative entries). This technique is most commonly used in recommender systems, and was made well known by the Netflix Prize. NMF aims to factor a data matrix $X$ into a product of two matrices:\n",
    "\n",
    "$$X \\approx AS $$\n",
    "\n",
    "where $X$ is a $n \\times m$ matrix, $A$ is a $n \\times k$ matrix, and $S$ is a $k \\times m$ matrix. $k$ is usually provided by the user, and symbolizes the number of distinct \"factors\" in the data. For example, if our data was the total productivity of a group of factories per hour for the past week, the number of factors $k$ would be the number of factories. Without prior knowledge the number of factors would be harder to pinpoint, and would have to be chosen using cross validation or something similar\n",
    "\n",
    "It's important to note that this problem does not have a unique solution, and we could end up with many different combinations of $A$ and $S$ that multiply to get a decent approximation of $X$. Even more, each pair of $A$ and $S$ can be scaled by any real number $\\alpha$ and $\\frac{1}{\\alpha}$ respectively to yield an infinite number of pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The natural question to ask now is how to determine $A$ and $S$ when given $X$ and $k$. One relatively simple method is to use alternating least squares, which is a generalization of the least squares method for simple linear regression. In simple linear regression, the goal is to solve the following equation for $x$:\n",
    "\n",
    "$$ Ax = b \\implies A^TAx = A^Tb \\implies x = (A)^{\\dagger}b\\$$\n",
    "\n",
    "This can be generalized for a product of matrices by picking a random $i^{th}$ column of $X$ and $S$, which we will denote $x_{:,i}$ and $s_{:,i}$, fixing $A$, and solving for $s_{:,i}$. Then by our previous equation $X \\approx AS$ we have \n",
    "\n",
    "$$ x_{:,i} \\approx As_{:,i}$$\n",
    "This yields the following update rule:\n",
    "\n",
    "$$ s_{:,i} := (A)^{\\dagger}x_{:,i}$$\n",
    "\n",
    "However, since we also want to solve for $A$ we need to sample a column of $A$ and fix $S$. To get the same linear form we do the following:\n",
    "\n",
    "$$x_{i,:} \\approx a_{i,:}S \\implies x_{i,:}^T \\approx S^Ta_{i,:}^T$$\n",
    "\n",
    "We switch to updating the rows of $A$ rather than the columns due to dimensionality, and get the following update rule:\n",
    "\n",
    "$$ a_{i,:}^T = (SS^T)^{-1}Sx_{i,:}^T $$\n",
    "\n",
    "We then repeat these updates until convergence or the number of iterations is fulfilled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   5.   0.   0.   5.   0.   0.   0.   5.   0. ]\n",
      " [ 9.  14.   0.   0.  14.   9.   9.   9.  14.   0. ]\n",
      " [ 5.  11.   0.   0.  11.   5.   5.   5.  11.   0. ]\n",
      " [ 3.   9.   0.   0.   9.   3.   3.   3.   9.   0. ]\n",
      " [ 2.  12.   0.   3.  12.   2.   2.   5.   9.   0. ]\n",
      " [ 1.   7.   0.   2.   7.   1.   1.   3.   5.   0. ]\n",
      " [ 0.   3.   0.   1.   3.   0.   0.   1.   2.   0. ]\n",
      " [ 0.   2.   0.   1.   2.   0.   0.   1.   1.   0. ]\n",
      " [ 0.   0.5  0.   0.   0.5  0.   0.   0.   0.5  0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "col1 = np.array([[0, 0, 9, 5, 3, 2, 1, 0, 0, 0, 0, 0]])\n",
    "col2 = np.array([[0, 0, 0, 0, 0, 3, 2, 1, 1, 0, 0, 0]])\n",
    "col3 = np.array([[0, 5, 5, 6, 6, 7, 4, 2, 1, 0.5, 0, 0]])\n",
    "\n",
    "factors = np.vstack((col1, col2, col3)).T\n",
    "weights = np.random.randint(0, 2, size=(3, 10))\n",
    "\n",
    "X = np.matmul(factors, weights)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the factor matrices $A$ and $S$ is typically done by filling a matrix of the right dimensions with randomized entries. However, to check that the ALS algorithm works properly we can initialize $A$ and $S$ close to our original factor matrices. The final $A$ and $S$ should yield a very low error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   5.   0.   0.   5.   0.   0.   0.   5.   0. ]\n",
      " [ 9.  14.   0.   0.  14.   9.   9.   9.  14.   0. ]\n",
      " [ 5.  11.   0.   0.  11.   5.   5.   5.  11.   0. ]\n",
      " [ 3.   9.   0.   0.   9.   3.   3.   3.   9.   0. ]\n",
      " [ 2.  12.   0.   3.  12.   2.   2.   5.   9.   0. ]\n",
      " [ 1.   7.   0.   2.   7.   1.   1.   3.   5.   0. ]\n",
      " [ 0.   3.   0.   1.   3.   0.   0.   1.   2.   0. ]\n",
      " [ 0.   2.   0.   1.   2.   0.   0.   1.   1.   0. ]\n",
      " [ 0.   0.5  0.   0.   0.5  0.   0.   0.   0.5  0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]\n",
      "[[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [-0.   5.   0.   0.   5.   0.  -0.   0.   5.   0. ]\n",
      " [ 9.  14.   0.   0.  14.   9.   9.   9.  14.   0. ]\n",
      " [ 5.  11.   0.  -0.  11.   5.   5.   5.  11.   0. ]\n",
      " [ 3.   9.   0.   0.   9.   3.   3.   3.   9.   0. ]\n",
      " [ 2.  12.   0.   3.  12.   2.   2.   5.   9.   0. ]\n",
      " [ 1.   7.   0.   2.   7.   1.   1.   3.   5.   0. ]\n",
      " [-0.   3.   0.   1.   3.  -0.  -0.   1.   2.   0. ]\n",
      " [-0.   2.   0.   1.   2.  -0.   0.   1.   1.   0. ]\n",
      " [-0.   0.5  0.   0.   0.5  0.  -0.   0.   0.5  0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]\n",
      "Relative error:  2.5770494327987753e-16\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "k = 3\n",
    "niter = 1000\n",
    "A = factors + 0.01*np.random.rand(12, 3)\n",
    "S = weights + 0.01*np.random.rand(3, 10)\n",
    "\n",
    "for i in np.arange(niter):\n",
    "    row = np.random.randint(X.shape[0])\n",
    "    col = np.random.randint(X.shape[1])\n",
    "    rowcol = np.random.randint(k)\n",
    "    S[:, col] = np.matmul(np.linalg.pinv(A), X[:, col])\n",
    "    A[row, :] = np.matmul(X[row, :], np.matmul(S.T, np.linalg.inv(np.matmul(S, S.T))))\n",
    "\n",
    "approx = np.matmul(A, S)\n",
    "print(X)\n",
    "print(np.round(approx, 2))\n",
    "print(\"Relative error: \", np.linalg.norm(X - approx) / np.linalg.norm(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the entries of our approximation $AS$ are pretty close to our data matrix, and the relative error is fairly low. Now that the algorithm works we can finalize it in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmfals(data, k, niter, reinit = 5):\n",
    "    # set to negative one so we can guarantee an update for the first init\n",
    "    finalerror = -1\n",
    "    \n",
    "    # need to compare final error to overall best and store the overall best\n",
    "    seqerror = np.empty(niter)\n",
    "    lowesterror = np.empty(1)\n",
    "    \n",
    "    # store overall best factor matrices\n",
    "    lbest = np.random.rand(data.shape[0], k)\n",
    "    rbest = np.random.rand(k, data.shape[1])\n",
    "    \n",
    "    for j in np.arange(reinit):\n",
    "        # randomly initialize the factor matrices\n",
    "        lfactor = np.random.rand(data.shape[0], k)\n",
    "        rfactor = np.random.rand(k, data.shape[1])\n",
    "\n",
    "        for i in np.arange(niter):\n",
    "            # sample random row or column\n",
    "            row = np.random.randint(data.shape[0])\n",
    "            col = np.random.randint(data.shape[1])\n",
    "            # perform linear reg update \n",
    "            rfactor[:, col] = np.matmul(np.linalg.pinv(lfactor), data[:, col])\n",
    "            lfactor[row, :] = np.matmul(data[row, :], np.matmul(rfactor.T, np.linalg.inv(np.matmul(rfactor, rfactor.T))))\n",
    "            # calculate error after update\n",
    "            seqerror[i] = np.linalg.norm(data - np.matmul(lfactor, rfactor)) / np.linalg.norm(data)\n",
    "        # update after first init\n",
    "        if (finalerror == -1):\n",
    "            lowesterror = seqerror\n",
    "            lbest = lfactor\n",
    "            rbest = rfactor\n",
    "        # if not first, only update if final error is lower than overall best\n",
    "        elif (finalerror > seqerror[niter - 1]):\n",
    "            finalerror = seqerror[niter - 1]\n",
    "            lowesterror = seqerror\n",
    "            lbest = lfactor\n",
    "            rbest = rfactor\n",
    "    return(lbest, rbest, lowesterror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [-0.    5.21  0.    0.    4.78 -0.   -0.   -0.    5.    0.  ]\n",
      " [ 9.   13.85  0.   -0.   14.17  9.    9.    9.   14.    0.  ]\n",
      " [ 5.   11.06  0.    0.   10.95  5.    5.    5.   11.    0.  ]\n",
      " [ 3.    9.13  0.   -0.    8.86  3.    3.    3.    9.    0.  ]\n",
      " [ 2.   12.07  0.    3.   11.93  2.    2.    5.    9.    0.  ]\n",
      " [ 1.    7.03  0.    2.    6.97  1.    1.    3.    5.    0.  ]\n",
      " [-0.    3.04  0.    1.    2.96 -0.   -0.    1.    2.    0.  ]\n",
      " [-0.    1.99  0.    1.    2.01 -0.   -0.    1.    1.    0.  ]\n",
      " [-0.    0.52  0.    0.    0.48 -0.   -0.   -0.    0.5   0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n",
      "[0.9220367  0.91425361 0.91351245 0.86254293 0.7929682  0.78072983\n",
      " 0.71107153 0.71107153 0.71052677 0.70625435 0.69377834 0.68167757\n",
      " 0.67951383 0.67920698 0.67920698 0.67764996 0.58584423 0.58567305\n",
      " 0.55999421 0.54277558 0.38195369 0.3773304  0.36081934 0.35839456\n",
      " 0.35817441 0.31793814 0.31536491 0.31472006 0.29000962 0.28237117\n",
      " 0.27434566 0.26807625 0.26807084 0.26386273 0.26313696 0.26278472\n",
      " 0.25822514 0.23702587 0.23593111 0.22636193 0.1052943  0.10089948\n",
      " 0.09582343 0.0931581  0.0931581  0.08697755 0.08517689 0.07559371\n",
      " 0.07516256 0.07471928 0.07145129 0.0707756  0.07053284 0.07006641\n",
      " 0.0700646  0.07000071 0.06999785 0.06822962 0.06822684 0.06433506\n",
      " 0.06347397 0.06335908 0.06309765 0.06042079 0.05520274 0.05413863\n",
      " 0.05413863 0.05413863 0.05402362 0.05402362 0.05393463 0.05181282\n",
      " 0.05142999 0.01674971 0.01627258 0.01626353 0.01528581 0.01279554\n",
      " 0.01075591 0.01073807 0.01009304 0.00992936 0.009928   0.00992408\n",
      " 0.00986323 0.00981731 0.00948874 0.00947215 0.00947215 0.00945563\n",
      " 0.00945563 0.00943291 0.00943291 0.00942535 0.00934588 0.00932952\n",
      " 0.00932952 0.00932946 0.00932946 0.00932882]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A, S, error = nmfals(X, 3, 100, 10)\n",
    "print(np.round(np.matmul(A,S), 2))\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the ALS algorithm does not always closely resemble the original data matrix in practice, as the random initializations of $A$ and $S$ can cause the resulting approximation to vary wildly even with multiple iterations. This makes sense, as there are many different factorizations that a matrix can have. While the factorized $A$ and $S$ don't form a matrix that matches $X$ closely, it did preserve the row and column of zeros that were present in $X$. \n",
    "\n",
    "One thing to note is we use the relative error to judge the quality of our approximation, which is the Frobenius norm of the difference between our original data matrix $X$ and the approximation $AS$ divided by the Frobenius norm of $X$. We do this rather than just use the raw error since factorizations can be scaled by multiplying $A$ by a number $r$ and $S$ by $\\frac{1}{r}$. This scaling also scales the error, hence the need for a relative metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Kaczmarz Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw previously that the algorithm to factorize $X$ has two main parts, one to pick our matrix column and row indices and one to take an iterative step towards the local optimum. The iterative step gives us the freedom to choose our favorite method to solve for $A$ and $S$, and rather than doing a traditional least squares method we can try applying the Randomized Kaczmarz(RK) method instead. This iterative step takes our randomly chosen row/column of either $A$ or $S$ and projects it towards the local optimum. It is equivalent to stochastic gradient descent with a specific step size when the matrix is positive definite.\n",
    "\n",
    "Our current system $AS = X$ is reduced to $As_{:,i} = x_{:,i}$ when a column is sampled. Our RK iterative step then samples a row of $A$ and corresponding entry $k$ of $s_{:,i}$ and would then be the following:\n",
    "\n",
    "$$s_{:,i}^{(j+1)} = s_{:,i}^{(j)} + \\frac{x_{k,i} - a_{k,:}^Ts_{:,i}}{\\lvert\\lvert{a_{k,:}}\\rvert\\rvert ^2}a_{k,:}$$\n",
    "\n",
    "Note that $j$ represents the current iteration of our RK method. This value can be explicitly chosen, making it an additional parameter in this algorithm. If we sampled a row of $S$ rather than a column of $A$, each step would instead be the following:\n",
    "\n",
    "$$a_{i,:}^{(j+1)} = a_{i,:}^{(j)} + \\frac{x_{i,k} - a_{i,:}^Ts_{:,k}}{\\lvert\\lvert{s_{:,k}}\\rvert\\rvert ^2}s_{:,k}$$\n",
    "\n",
    "To summarize, we start by randomly sampling a row/column to reduce to a linear system(as usual). We then proceed to take RK steps, with each step sampling a random row and entry and updating. The number of steps before resampling our linear system can be provided as a parameter. \n",
    "\n",
    "We can also perform quick sanity check by initializing $A$ and $S$ close to the original factor matrices like we did with the ALS method. It's important to note that dividing by a vector norm means that we have to avoid sampling the rows/columns of our factor matrices that are all zero. This can be done by doing a weighted sample based on the norms of the rows/columns where a row with twice the magnitude of another will be sampled twice as often and a row with zero norm will never be sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.07319144 0.25522726 0.18185286 0.14116653 0.1721685\n",
      " 0.09964076 0.04140333 0.02803019 0.00731914 0.         0.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 2., 4.])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weightsample(data, mode):\n",
    "    prob = np.linalg.norm(data, axis=mode)\n",
    "    return(prob / sum(prob))\n",
    "print(weightsample(X, 1))\n",
    "row = np.random.choice(X.shape[0], p = weightsample(X, 1))\n",
    "factors[row, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   5.   0.   0.   5.   0.   0.   0.   5.   0. ]\n",
      " [ 9.  14.   0.   0.  14.   9.   9.   9.  14.   0. ]\n",
      " [ 5.  11.   0.   0.  11.   5.   5.   5.  11.   0. ]\n",
      " [ 3.   9.   0.   0.   9.   3.   3.   3.   9.   0. ]\n",
      " [ 2.  12.   0.   3.  12.   2.   2.   5.   9.   0. ]\n",
      " [ 1.   7.   0.   2.   7.   1.   1.   3.   5.   0. ]\n",
      " [ 0.   3.   0.   1.   3.   0.   0.   1.   2.   0. ]\n",
      " [ 0.   2.   0.   1.   2.   0.   0.   1.   1.   0. ]\n",
      " [ 0.   0.5  0.   0.   0.5  0.   0.   0.   0.5  0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]\n",
      "[[-0.   -0.    0.   -0.   -0.02 -0.    0.   -0.   -0.    0.  ]\n",
      " [ 0.89  0.29 -0.01 -0.06  5.12 -0.36 -0.51  0.55 -0.32 -0.01]\n",
      " [ 9.    8.67 -0.03 -0.   26.76  7.24  2.8   8.96  6.89 -0.  ]\n",
      " [ 7.87  8.94 -0.01  0.08 16.69  9.47  4.73  8.79  8.32  0.02]\n",
      " [ 4.61  4.85 -0.01  0.24 13.98  2.94  0.94  4.44  4.65 -0.  ]\n",
      " [ 5.48  6.98  0.01  0.93 16.69  1.83  0.05  5.    8.65  0.01]\n",
      " [ 3.65  3.78 -0.    0.42 13.85  0.34 -0.64  3.    4.03 -0.01]\n",
      " [ 2.01  2.75  0.01  0.41  5.85  0.62  0.    1.84  3.6   0.  ]\n",
      " [ 0.09  2.    0.03  0.86 -1.24 -1.41 -0.85 -0.07  4.74  0.02]\n",
      " [ 0.08  0.32  0.    0.08 -0.21  0.09  0.06  0.12  0.59  0.  ]\n",
      " [-0.    0.    0.    0.   -0.01  0.    0.   -0.    0.01  0.  ]\n",
      " [-0.   -0.    0.    0.   -0.02  0.    0.   -0.    0.    0.  ]]\n",
      "0.5396653015850852\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "k = 3\n",
    "niter = 100\n",
    "factors = factors \n",
    "weights = weights \n",
    "X = np.round(np.matmul(factors, weights), 2)\n",
    "\n",
    "A = factors + 0.01*np.random.rand(12, 3)\n",
    "S = weights + 0.01*np.random.rand(3, 10)\n",
    "\n",
    "kacziters = 1000\n",
    "for j in np.arange(niter):\n",
    "    row = np.random.choice(X.shape[0], p = weightsample(X, 1))\n",
    "    col = np.random.choice(X.shape[1], p = weightsample(X, 0))\n",
    "    for i in np.arange(kacziters):\n",
    "        kaczrow = np.random.randint(len(X[:, col]))\n",
    "        kaczcol = np.random.randint(len(X[row, :]))\n",
    "        S[:, col] = S[:, col] + (X[kaczrow, col] - np.matmul(A[kaczrow, :], S[:, col])) / (np.linalg.norm(A[kaczrow, :])**2) * A[kaczrow, :]\n",
    "        A[row, :] = A[row, :] + (X[row, kaczcol] - np.matmul(A[row, :], S[:, kaczcol])) / (np.linalg.norm(S[:, kaczcol])**2) * S[:, kaczcol] \n",
    "\n",
    "approx = np.matmul(A, S)\n",
    "print(X)\n",
    "print(np.round(approx, 2))\n",
    "print(np.linalg.norm(X - approx) / np.linalg.norm(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RK method's relative error is nowhere near comparable to that of ALS even when the number of RK iterations is in the thousands due to the presence of more negative entries. \n",
    "\n",
    "The finalized function is largely similar to the ALS one, except the ALS iterative step is substituted for the RK iteration loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightsample(data, mode):\n",
    "    prob = np.linalg.norm(data, axis=mode)\n",
    "    return(prob / sum(prob))\n",
    "\n",
    "def nmfrk(data, k, niter, kacziter, reinit = 5):\n",
    "    # set to negative one so we can guarantee an update for the first init\n",
    "    finalerror = -1\n",
    "    \n",
    "    # need to compare final error to overall best and store the overall best\n",
    "    seqerror = np.empty(niter)\n",
    "    lowesterror = np.empty(1)\n",
    "    \n",
    "    # store overall best factor matrices\n",
    "    lbest = np.random.rand(data.shape[0], k)\n",
    "    rbest = np.random.rand(k, data.shape[1])\n",
    "    \n",
    "    for j in np.arange(reinit):\n",
    "        # randomly initialize the factor matrices\n",
    "        lfactor = np.random.rand(data.shape[0], k)\n",
    "        rfactor = np.random.rand(k, data.shape[1])\n",
    "        \n",
    "        # outer loop for number of iterations \n",
    "        for i in np.arange(niter):\n",
    "            \n",
    "            # weighted sampling of row and column from data matrix\n",
    "            row = np.random.choice(data.shape[0], p = weightsample(data, 1))\n",
    "            col = np.random.choice(data.shape[1], p = weightsample(data, 0))\n",
    "            \n",
    "            # inner loop for number of RK iterations\n",
    "            for j in np.arange(kacziter):\n",
    "                # sample index for entry of data matrix\n",
    "                kaczrow = np.random.randint(len(data[:, col]))\n",
    "                kaczcol = np.random.randint(len(data[row, :]))\n",
    "                # compute RK step\n",
    "                lfactor[row, :] = lfactor[row, :] + (data[row, kaczcol] - np.matmul(lfactor[row, :], rfactor[:, kaczcol])) / (np.linalg.norm(rfactor[:, kaczcol])**2) * rfactor[:, kaczcol] \n",
    "                rfactor[:, col] = rfactor[:, col] + (data[kaczrow, col] - np.matmul(lfactor[kaczrow, :], rfactor[:, col])) / (np.linalg.norm(lfactor[kaczrow, :])**2) * lfactor[kaczrow, :]\n",
    "            # calculate error after update\n",
    "            seqerror[i] = np.linalg.norm(data - np.matmul(lfactor, rfactor)) / np.linalg.norm(data)\n",
    "        # update after first init\n",
    "        if (finalerror == -1):\n",
    "            lowesterror = seqerror\n",
    "            lbest = lfactor\n",
    "            rbest = rfactor\n",
    "        # if not first, only update if final error is lower than overall best\n",
    "        elif (finalerror > seqerror[niter - 1]):\n",
    "            finalerror = seqerror[niter - 1]\n",
    "            lowesterror = seqerror\n",
    "            lbest = lfactor\n",
    "            rbest = rfactor\n",
    "    return(lbest, rbest, lowesterror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   5.   0.   0.   5.   0.   0.   0.   5.   0. ]\n",
      " [ 9.  14.   0.   0.  14.   9.   9.   9.  14.   0. ]\n",
      " [ 5.  11.   0.   0.  11.   5.   5.   5.  11.   0. ]\n",
      " [ 3.   9.   0.   0.   9.   3.   3.   3.   9.   0. ]\n",
      " [ 2.  12.   0.   3.  12.   2.   2.   5.   9.   0. ]\n",
      " [ 1.   7.   0.   2.   7.   1.   1.   3.   5.   0. ]\n",
      " [ 0.   3.   0.   1.   3.   0.   0.   1.   2.   0. ]\n",
      " [ 0.   2.   0.   1.   2.   0.   0.   1.   1.   0. ]\n",
      " [ 0.   0.5  0.   0.   0.5  0.   0.   0.   0.5  0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]]\n",
      "[[   0.24   -0.09    0.42    0.38    3.31    0.26    0.04    0.49   -0.64\n",
      "     0.5 ]\n",
      " [  -0.18    2.26    0.01    0.79    3.17    0.57    0.04    0.74    4.44\n",
      "     0.13]\n",
      " [  14.04  -78.37   40.14   -0.     83.13    4.92    1.13  -10.95 -156.28\n",
      "    36.73]\n",
      " [   3.18   -4.48    3.24    2.41   29.81    1.3     0.36    5.    -15.3\n",
      "     4.13]\n",
      " [   9.12    6.54   -5.49    7.21   83.39    0.48    0.96   25.93  -17.68\n",
      "    -0.  ]\n",
      " [   5.13    6.94   -1.56    6.19   58.31    2.      0.68   16.13   -3.53\n",
      "     1.69]\n",
      " [   1.32    1.57    0.55    2.     17.35    1.      0.21    3.94   -0.81\n",
      "     1.29]\n",
      " [   0.05    3.     -0.84    0.83    4.34    0.32    0.05    1.76    4.64\n",
      "    -0.48]\n",
      " [   0.15    0.79   -0.09    0.4     2.84    0.19    0.03    0.79    0.89\n",
      "     0.06]\n",
      " [   0.01    0.22   -0.02    0.09    0.5     0.05    0.01    0.14    0.33\n",
      "     0.  ]\n",
      " [  -0.04   -0.49    0.43   -0.01   -0.14    0.11    0.     -0.39   -0.52\n",
      "     0.33]\n",
      " [   0.11    0.      0.6     0.4     2.74    0.36    0.03    0.19    0.\n",
      "     0.6 ]]\n",
      "4.946523351357186\n"
     ]
    }
   ],
   "source": [
    "A, S, error = nmfrk(X, k = 3, niter = 100, kacziter = 1000, reinit = 5)\n",
    "\n",
    "approx = np.matmul(A, S)\n",
    "print(X)\n",
    "print(np.round(approx, 2))\n",
    "print(np.linalg.norm(X - approx) / np.linalg.norm(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to notice that RK is more inclined to have negative entries, which further increases the error as other entries grow larger to compensate. We can address this problem by introducing a nonnegativity constraint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
